#!/usr/bin/env python3
"""
RSSåé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
AI/DXé–¢é€£ã®æŠ€è¡“è¨˜äº‹ã‚’åé›†ã—ã¦queue.ymlã«ä¿å­˜
"""

import feedparser
import yaml
from datetime import datetime
import os
from dateutil import parser
import sys

# è¨­å®š
RSS_FEEDS = [
    'https://techcrunch.com/category/artificial-intelligence/feed/',
    'https://dev.to/feed/tag/ai',
    'https://zenn.dev/topics/ai/feed',
    'https://www.publickey1.jp/atom.xml',  # æ—¥æœ¬èªã‚½ãƒ¼ã‚¹
    'https://qiita.com/tags/ai/feed',  # æ—¥æœ¬èªã‚½ãƒ¼ã‚¹è¿½åŠ 
]

QUEUE_FILE = 'queue.yml'

def load_queue():
    """æ—¢å­˜ã®ã‚­ãƒ¥ãƒ¼ã‚’èª­ã¿è¾¼ã¿"""
    if os.path.exists(QUEUE_FILE):
        with open(QUEUE_FILE, 'r', encoding='utf-8') as f:
            data = yaml.safe_load(f)
            return data if data else []
    return []

def save_queue(items):
    """ã‚­ãƒ¥ãƒ¼ã‚’ä¿å­˜"""
    with open(QUEUE_FILE, 'w', encoding='utf-8') as f:
        yaml.dump(items, f, allow_unicode=True, default_flow_style=False, sort_keys=False)

def normalize_date(date_str):
    """æ—¥ä»˜æ–‡å­—åˆ—ã‚’æ­£è¦åŒ–"""
    if not date_str:
        return datetime.now().isoformat()
    try:
        return parser.parse(date_str).isoformat()
    except Exception as e:
        print(f"Date parsing error: {e}")
        return datetime.now().isoformat()

def is_relevant_content(title, summary=""):
    """AI/DXé–¢é€£ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
    keywords = [
        'ai', 'artificial intelligence', 'machine learning', 'deep learning',
        'chatgpt', 'gpt', 'llm', 'automation', 'dx', 'digital transformation',
        'openai', 'claude', 'gemini', 'copilot', 'python', 'javascript',
        'äººå·¥çŸ¥èƒ½', 'AI', 'DX', 'ãƒ‡ã‚¸ã‚¿ãƒ«å¤‰é©', 'è‡ªå‹•åŒ–', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°',
        'ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ', 'ãƒ­ãƒ¼ã‚³ãƒ¼ãƒ‰', 'ãƒãƒ¼ã‚³ãƒ¼ãƒ‰'
    ]
    
    text = (title + " " + summary).lower()
    return any(keyword.lower() in text for keyword in keywords)

def collect_feeds():
    """RSSåé›†ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    queue = load_queue()
    existing_urls = {item.get('url') for item in queue if 'url' in item}
    new_items = []
    
    print(f"ğŸ“¡ RSSåé›†é–‹å§‹: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"æ—¢å­˜ã‚¢ã‚¤ãƒ†ãƒ æ•°: {len(queue)}")
    
    for i, feed_url in enumerate(RSS_FEEDS, 1):
        try:
            print(f"\n[{i}/{len(RSS_FEEDS)}] Fetching: {feed_url}")
            
            # User-Agentã‚’è¨­å®šã—ã¦ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—
            feed = feedparser.parse(feed_url)
            
            if hasattr(feed, 'bozo') and feed.bozo:
                print(f"  âš ï¸  Warning: Feed parsing issue for {feed_url}")
            
            # ãƒ•ã‚£ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            feed_title = feed.feed.get('title', 'Unknown Source')
            print(f"  ğŸ“° Source: {feed_title}")
            
            added_count = 0
            for entry in feed.entries[:5]:  # å„ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰æœ€æ–°5ä»¶
                if entry.link not in existing_urls:
                    # é–¢é€£æ€§ãƒã‚§ãƒƒã‚¯
                    summary = entry.get('summary', '')
                    if not is_relevant_content(entry.title, summary):
                        continue
                    
                    # æ—¥ä»˜å–å¾—
                    pub_date = entry.get('published', entry.get('updated', ''))
                    
                    new_item = {
                        'id': datetime.now().strftime('%Y%m%d') + f'-{len(queue) + len(new_items) + 1:03d}',
                        'title': entry.title.strip(),
                        'url': entry.link,
                        'source': feed_title,
                        'summary': summary[:200] + '...' if len(summary) > 200 else summary,
                        'published_date': normalize_date(pub_date),
                        'collected_at': datetime.now().isoformat(),
                        'status': 'pending',
                        'score': None,  # å¾Œã§æ‰‹å‹•è©•ä¾¡
                        'memo': ''      # å¾Œã§æ‰‹å‹•è¿½è¨˜
                    }
                    new_items.append(new_item)
                    added_count += 1
                    print(f"  âœ… Added: {entry.title[:60]}...")
                    
            print(f"  ğŸ“Š Added {added_count} new items from this feed")
                    
        except Exception as e:
            print(f"  âŒ Error fetching {feed_url}: {e}")
    
    if new_items:
        queue.extend(new_items)
        save_queue(queue)
        print(f"\nğŸ‰ åé›†å®Œäº†ï¼")
        print(f"æ–°è¦è¿½åŠ : {len(new_items)} ä»¶")
        print(f"ç·ã‚­ãƒ¥ãƒ¼æ•°: {len(queue)} ä»¶")
    else:
        print(f"\nğŸ“­ æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
    
    return len(new_items)

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    try:
        new_count = collect_feeds()
        
        if new_count > 0:
            print(f"\nğŸ“‹ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
            print(f"1. queue.yml ã‚’ç¢ºèªã—ã¦å†…å®¹ã‚’ãƒã‚§ãƒƒã‚¯")
            print(f"2. å…¬é–‹ã—ãŸã„è¨˜äº‹ã® status ã‚’ 'ready' ã«å¤‰æ›´")
            print(f"3. score (1-10) ã¨ memo ã‚’è¿½è¨˜")
            print(f"4. python3 scripts/create_post.py ã§è¨˜äº‹ç”Ÿæˆ")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  åé›†ã‚’ä¸­æ–­ã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

if __name__ == '__main__':
    main()